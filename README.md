Variational Autoencoder (VAE) Implementation using Keras: This project demonstrates the implementation of a Variational Autoencoder (VAE) using Keras. The VAE is trained on the MNIST dataset and is capable of generating new handwritten digits by sampling from the latent space.

Introduction: A Variational Autoencoder (VAE) is a generative model that learns to encode data into a latent space and decode from that latent space back to the original data. This project uses the MNIST dataset, consisting of handwritten digits, to train the VAE. The implementation includes creating the encoder and decoder networks, training the model, and visualizing the results.

Features:

Encoder and Decoder networks built using Keras.
Latent space sampling.
Training and validation on the MNIST dataset.
Visualization of the latent space and generated images.
Requirements

Python 3.x+
Keras
TensorFlow
NumPy
Matplotlib
Usage:

Run the script to train the VAE:

python3 vae.py
The script will load the MNIST dataset, preprocess the data, build the VAE model, and train it.

After training, the script will generate and save two plots:

Latent Space Visualization: Displays the distribution of the latent space.
Generated Images: Shows images generated by sampling from the latent space.
Results: Latent Space Visualization- The plot shows the distribution of the MNIST digits in the learned 2D latent space. Each point represents a digit, colored according to its true label.

Generated Images- The plot displays images generated by the decoder by sampling from the latent space grid. The axes represent the dimensions of the latent space.

Feel free to modify the code and experiment with different architectures and datasets. Contributions are welcome!
